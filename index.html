<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities.">
  <meta property="og:title" content="MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities" />
  <meta property="og:description"
    content="Transform any pretrained LLM into a unified model for representation learning, text infilling, and text generation.">
  <meta property="og:url" content="https://arxiv.org/abs/2412.01826" />
  <meta property="og:image" content="static/images/MAGNET_teaser.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />

  <meta name="twitter:title" content="MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities">
  <meta name="twitter:description" content="Transform any pretrained LLM into a unified model for representation learning, text infilling, and text generation.">
  <meta name="twitter:image" content="static/images/MAGNET_teaser.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords"
    content="computer vision, visual query localization, region-based representations, zero-shot localization">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>MAGNET</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <style>
    table {
      border-top: 1px solid #000;
      border-bottom: 1px solid #000;
      border-collapse: collapse;
      width: 100%;
    }
    th, td {
      text-align: center;
      padding: 8px;
      border: none; /* remove individual cell borders */
    }
    .section-header {
      font-weight: bold;
      background-color: #f0f0f0;
    }

    tr.highlight {
      background-color: #e8f5e8;
      font-weight: 600;
    }
    .uniform-gif {
      width: 280px;
      height: 200px;
      object-fit: cover;
    }
    .small-icon {
      width: 64px;
      height: 64px;
      object-fit: contain;
      margin-bottom: 0.5em;
    }
    .stacked {
      display: flex;
      flex-direction: column;
      align-items: center;
    }
  </style>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities</h1>
            <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://savya08.github.io/" target="_blank"><b>Savya Khosla</b></a><sup>1,2</sup>,
              <a href="https://adititiwari19.github.io/" target="_blank"><b>Aditi Tiwari</b></a><sup>2</sup>,
              <a href="https://kushalkafle.com/" target="_blank"><b>Kushal Kafle</b></a><sup>1</sup>,
              <a href="https://sjenni.github.io/" target="_blank"><b>Simon Jenni</b></a><sup>1</sup>,
              <a href="https://hdzhao.github.io/" target="_blank"><b>Handong Zhao</b></a><sup>1</sup>,
              <a href="https://research.adobe.com/person/john-collomosse/" target="_blank"><b>John Collomosse</b></a><sup>1</sup>,
              <a href="https://jshi31.github.io/jingshi/" target="_blank"><b>Jing Shi</b></a><sup>1</sup>
            </span><br>
            <span class="author-block" style="color: #000000;"><sup>1</sup>Adobe Research &emsp; <sup>2</sup>University of Illinois Urbana-Champaign</span><br>
          </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2501.08648" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="mailto:savyak2@illinois.edu" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-envelope"></i>
                    </span>
                    <span>Email</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <img src="figures/teaser.png" alt="MAGNET teaser" class="center" style="width: 900px;">
          <div class="content">
            <p>
              Traditionally, LLMs are trained for text generation using unidirectional attention (depicted by black lines), 
              whereas text encoders are trained for representation learning using bidirectional attention (depicted by gray lines). 
              <br>
              <b>MAGNET adapts the attention mechanism of LLMs to 
              combine both unidirectional and bidirectional attention, enhancing them with representation learning and infilling 
              capabilities, while retaining their core generative functions.</b>
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title is-3">Abstract</h3>
          <div class="content has-text-justified">
            <p>
              While originally designed for unidirectional generative modeling, decoder-only large language models (LLMs) 
              are increasingly being adapted for bidirectional modeling. However, unidirectional and bidirectional models 
              are typically trained separately with distinct objectives (generation and representation learning). This 
              separation overlooks the opportunity for developing a more versatile language model and for these objectives 
              to complement each other. In this work, we propose MAGNET, a method for adapting decoder-only LLMs to generate 
              robust representations and infill missing text spans. MAGNET employs three self-supervised training objectives 
              and introduces an attention mechanism that combines bidirectional and causal attention, enabling unified training 
              across all objectives. Our results demonstrate that LLMs adapted with MAGNET (1) surpass strong text encoders 
              on token-level and sentence-level representation learning tasks, (2) generate contextually appropriate text 
              infills by leveraging past and future contexts, (3) perform open-ended text generation without excessive 
              repetition of words or phrases, and (4) preserve the knowledge and reasoning capability gained by the LLM 
              during pretraining.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Method -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Method Overview</h2>
          <div class="content">
            <h4>Modify the Attention</h4>
            <p>
              We modify the attention mechanism of the LLM by updating the attention mask to combine causal and bidirectional 
              attention.
              <br>
              <li>
                All <b><span style="color:rgb(72, 114, 212);">context tokens</span></b> (shown in blue) attend to all other 
                context tokens in the sequnce.
              </li>
              <li>
                All <b><span style="color:rgb(2, 195, 89);">span tokens</span></b> (shown in green) have causal attention 
                among themselves and also attend to all context tokens.
              </li>
            </p>
          </div>
          <img src="figures/attention.png" alt="MAGNET attention" class="center" style="width: 800px;">
        </div>
      </div>
    </div>

    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <div class="content">
            <br>
            <h4>Fine-Tune the Model</h4>
            <p>
              We fine-tune the model using three self-supervised learning objectives, enabling it to understand and leverage the 
              modified attention mechanism. Fine-tuning is performed with LoRA for efficiency.
            </p>
          </div>
          <img src="figures/objectives.png" alt="MAGNET objectives" class="center" style="width: 850px;">
        </div>
      </div>
    </div>
  </section>

  <!-- Results -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Results</h2>
          <p>
            <br>
          </p>

          <div class="content">
            <details>
              <summary style="cursor: pointer; font-weight: bold; font-size: 1.1em; margin-bottom: 1em;">
                Word-Level Representation Learning
              </summary>

              <p>
                MAGNET outperforms LLM2Vec, demonstrating the benefits of unified training for representation learning. 
                MAGNET-adapted LLaMa 2-7B also outperforms strong encoders.
              </p>

              <table cellspacing="0" cellpadding="4">
                <thead>
                  <tr>
                    <th>Model</th>
                    <th>Chunking</th>
                    <th>NER</th>
                    <th>POS-Tags</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="section-header" colspan="4">Encoder Models</td>
                  </tr>
                  <tr>
                    <td>BERT-Large</td>
                    <td>71.77</td>
                    <td>90.09</td>
                    <td>75.12</td>
                  </tr>
                  <tr>
                    <td>XLNet-Large</td>
                    <td>79.70</td>
                    <td>93.67</td>
                    <td>83.02</td>
                  </tr>
                  <tr>
                    <td>DeBERTa-Large</td>
                    <td>85.74</td>
                    <td>94.97</td>
                    <td>86.49</td>
                  </tr>
                  <tr>
                    <td>StructBERT-Large</td>
                    <td>89.99</td>
                    <td>97.31</td>
                    <td>90.86</td>
                  </tr>

                  <tr>
                    <td class="section-header" colspan="4">Llama 2 Models</td>
                  </tr>
                  <tr>
                    <td>Llama-2-7B</td>
                    <td>88.23</td>
                    <td>96.59</td>
                    <td>91.53</td>
                  </tr>
                  <tr>
                    <td>LLM2Vec</td>
                    <td>89.66</td>
                    <td>96.05</td>
                    <td>90.53</td>
                  </tr>
                  <tr>
                    <td>LLM2Vec[MNTP]</td>
                    <td>91.61</td>
                    <td>97.16</td>
                    <td>92.61</td>
                  </tr>
                  <tr style="background-color: #e8f5e8; font-weight: 600;">
                    <td>MAGNET</td>
                    <td>92.64</td>
                    <td>98.31</td>
                    <td>93.34</td>
                  </tr>
                </tbody>
              </table>
            </details>
          </div>

          <div class="content">
            <details>
              <summary style="cursor: pointer; font-weight: bold; font-size: 1.1em; margin-bottom: 1em;">
                Sentence-Level Representation Learning
              </summary>

              <p>
                MAGNET also outperforms LLM2Vec and Echo Embeddings on sentence-level representation learning, despite these 
                baselines being trained specifically for text encoding.
              </p>

              <table cellspacing="0" cellpadding="4">
                <thead>
                  <tr>
                    <th>Model</th>
                    <th>STS12</th>
                    <th>STS13</th>
                    <th>STS14</th>
                    <th>STS15</th>
                    <th>STS16</th>
                    <th>STS-B</th>
                    <th>SICK-R</th>
                    <th>Avg</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="section-header" colspan="9">Encoder models (finetuned using SimCSE)</td>
                  </tr>
                  <tr>
                    <td>BERT-Base</td>
                    <td>68.40</td>
                    <td>82.41</td>
                    <td>74.38</td>
                    <td>80.91</td>
                    <td>78.56</td>
                    <td>76.85</td>
                    <td>72.23</td>
                    <td>76.25</td>
                  </tr>
                  <tr>
                    <td>RoBERTa-Base</td>
                    <td>70.16</td>
                    <td>81.77</td>
                    <td>73.24</td>
                    <td>81.36</td>
                    <td>80.65</td>
                    <td>80.22</td>
                    <td>68.56</td>
                    <td>76.57</td>
                  </tr>
                  <tr>
                    <td>RoBERTa-Large</td>
                    <td><b>72.86</b></td>
                    <td>83.99</td>
                    <td>75.62</td>
                    <td><b>84.77</b></td>
                    <td><b>81.80</b></td>
                    <td>81.98</td>
                    <td>71.26</td>
                    <td>78.90</td>
                  </tr>
              
                  <tr>
                    <td class="section-header" colspan="9">Llama 2 models</td>
                  </tr>
                  <tr>
                    <td>Llama-2-7B</td>
                    <td>50.98</td>
                    <td>74.02</td>
                    <td>62.86</td>
                    <td>67.09</td>
                    <td>71.03</td>
                    <td>63.56</td>
                    <td>67.22</td>
                    <td>65.25</td>
                  </tr>
                  <tr>
                    <td>Echo Embeddings</td>
                    <td>52.40</td>
                    <td>72.40</td>
                    <td>61.24</td>
                    <td>72.67</td>
                    <td>73.51</td>
                    <td>65.73</td>
                    <td>64.39</td>
                    <td>66.05</td>
                  </tr>
                  <tr>
                    <td>LLM2Vec</td>
                    <td>65.39</td>
                    <td>79.26</td>
                    <td>72.98</td>
                    <td>82.72</td>
                    <td>81.02</td>
                    <td>78.32</td>
                    <td>71.77</td>
                    <td>75.92</td>
                  </tr>
                  <tr style="background-color: #e8f5e8;">
                    <td><b>MAGNET</b></td>
                    <td>67.98</td>
                    <td><b>84.66</b></td>
                    <td><b>77.67</b></td>
                    <td>84.17</td>
                    <td>79.44</td>
                    <td><b>82.88</b></td>
                    <td><b>78.77</b></td>
                    <td><b>79.36</b></td>
                  </tr>
                </tbody>
              </table>

            </details>
          </div>

          <div class="content">
            <details>
              <summary style="cursor: pointer; font-weight: bold; font-size: 1.1em; margin-bottom: 1em;">
                Text Infilling
              </summary>

              <p>
                MAGNET enables bidirectional context access, making it particularly effective for text infilling.
              </p>

              <table cellspacing="0" cellpadding="4" style="border-collapse: collapse; width: 100%;">
                <thead>
                  <tr>
                    <th style="text-align: center; padding: 8px;">Method</th>
                    <th style="text-align: center; padding: 8px;">Infilled Line</th>
                  </tr>
                </thead>
                <tbody>
                  <!-- Example 1 -->
                  <tr>
                    <td colspan="2" style="padding: 8px; font-style: italic; font-weight: 600; background-color: #e8e8e8;">
                      John was at the gym. <span>_____</span> He couldn’t get the barbell off his chest. He was too embarrassed to call for help. John suffocated as the weights crushed his chest.
                    </td>
                  </tr>
                  <tr>
                    <td style="padding: 8px;">Uni-Llama</td>
                    <td style="padding: 8px;">He was on the treadmill, running at a steady pace.</td>
                  </tr>
                  <tr>
                    <td style="padding: 8px;">FS-Llama</td>
                    <td style="padding: 8px;">He was doing bench presses.</td>
                  </tr>
                  <tr>
                    <td style="padding: 8px;">MAGNET</td>
                    <td style="padding: 8px;">John was doing bench presses when he accidentally dropped the barbell on top.</td>
                  </tr>
              
                  <!-- Example 2 -->
                  <tr>
                    <td colspan="2" style="padding: 8px; font-style: italic; font-weight: 600; background-color: #e8e8e8;">
                      Toni took a trip to the local fair with her family. <span>_____</span> The balloon was red and star shaped. Unfortunately it slipped out of her hands. She was sad to see it go.
                    </td>
                  </tr>
                  <tr>
                    <td style="padding: 8px;">Uni-Llama</td>
                    <td style="padding: 8px;">She won a prize for her drawing of a cat.</td>
                  </tr>
                  <tr>
                    <td style="padding: 8px;">FS-Llama</td>
                    <td style="padding: 8px;">She won it in a game of darts.</td>
                  </tr>
                  <tr>
                    <td style="padding: 8px;">MAGNET</td>
                    <td style="padding: 8px;">She was so excited to see the balloon vendor and bought a red one.</td>
                  </tr>
              
                  <!-- Example 3 -->
                  <tr>
                    <td colspan="2" style="padding: 8px; font-style: italic; font-weight: 600; background-color: #e8e8e8;">
                      I was resting on my couch. <span>_____</span> They raised their water guns. I immediately ran. I went outside and sprayed them with the hose when they followed.
                    </td>
                  </tr>
                  <tr>
                    <td style="padding: 8px;">Uni-Llama</td>
                    <td style="padding: 8px;">I was feeling a bit under the weather, so I decided to take a break.</td>
                  </tr>
                  <tr>
                    <td style="padding: 8px;">FS-Llama</td>
                    <td style="padding: 8px;">My neighbor’s kids were playing tag in the backyard.</td>
                  </tr>
                  <tr>
                    <td style="padding: 8px;">MAGNET</td>
                    <td style="padding: 8px;">I heard a noise and looked out the window and saw two boys with water guns.</td>
                  </tr>
                </tbody>
              </table>
              
              <p>
                <br>
                Human evaluators also rate MAGNET's generated text as more coherent compared to other setups.
                The evaluation scores denote the percentage of infillings that were considered contextually appropriate by human evaluators.
              </p>

              <table cellspacing="0" cellpadding="4">
                <thead>
                  <tr>
                    <th>Method</th>
                    <th>Human Evaluation Score</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>Unidirectional Llama-2-7B</td>
                    <td>53.5</td>
                  </tr>
                  <tr>
                    <td>Zero-Shot Setup</td>
                    <td>5.5</td>
                  </tr>
                  <tr>
                    <td>Five-Shot Setup</td>
                    <td>54.5</td>
                  </tr>
                  <tr style="background-color: #e8f5e8; font-weight: 600;">
                    <td>MAGNET</td>
                    <td>62.0</td>
                  </tr>
                </tbody>
              </table>

            </details>
          </div>

          <div class="content">
            <details>
              <summary style="cursor: pointer; font-weight: bold; font-size: 1.1em; margin-bottom: 1em;">
                Repetition Problem
              </summary>

              <p>
                Unlike other encoder models like BERT and LLM2Vec, MAGNET retains the ability to generate coherent and 
                non-repetitive text.
              </p>

              <table cellspacing="0" cellpadding="4" style="border-collapse: collapse; width: 100%;">
                <thead>
                  <tr>
                    <th style="text-align: center; padding: 8px;">Model</th>
                    <th style="text-align: center; padding: 8px;">Generated Text</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td colspan="2" style="padding: 8px;">
                      <b>Initial phrase:</b> <span style="background-color: yellow;">The film was well received</span>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding: 8px;">BERT</td>
                    <td style="padding: 8px;">
                      <span style="background-color: yellow;">The film was well received</span> and the " " " " " " " " " " " " " " " " " are " " are " are " are are " are are the are the the the the the the the the the the the so so and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and ...
                    </td>
                  </tr>
                  <tr>
                    <td style="padding: 8px;">LLM2Vec</td>
                    <td style="padding: 8px;">
                      <span style="background-color: yellow;">The film was well received</span> by critics and audiences alike.<br>
                      The film was well received by critics and audiences alike.<br>
                      The film was well received by critics and audiences alike.<br>
                      The film was well received by critics and audiences alike.<br>
                      The film was well received by critics and audiences alike.<br>
                      The film was well received by critics and audiences alike.<br>
                      The film was well received by critics and audiences alike...
                    </td>
                  </tr>
                  <tr style="background-color: #e8e8e8;">
                    <td style="padding: 8px;">MAGNET</td>
                    <td style="padding: 8px;">
                      <span style="background-color: yellow;">The film was well received</span> by critics and audiences alike and was nominated for several awards including the Academy Award for Best Visual Effects and the Saturn Award for Best Science Fiction Film. The film was also a commercial success and grossed over $100 million at the box office.<br><br>
                      == Plot ==<br>
                      In the year 2018, a meteorite crashes into the Pacific Ocean, causing a massive tsunami that destroys most of the world's coastal cities. The survivors of the disaster band together ...
                    </td>
                  </tr>
                </tbody>
              </table>

            </details>
          </div>

          <div class="content">
            <details>
              <summary style="cursor: pointer; font-weight: bold; font-size: 1.1em; margin-bottom: 1em;">
                Knowledge and Reasoning Tasks
              </summary>

              <p>
                MAGNET maintains the knowledge and reasoning abilities acquired during LLM pretraining.
              </p>

              <table cellspacing="0" cellpadding="4" style="border-collapse: collapse; width: 100%; text-align: center;">
                <thead>
                  <tr>
                    <th rowspan="2" style="padding: 8px; text-align: left;">Model</th>
                    <th rowspan="2" style="padding: 8px;">HellaSwag</th>
                    <th rowspan="2" style="padding: 8px;">BBH</th>
                    <th colspan="2" style="padding: 8px;">ARC</th>
                    <th rowspan="2" style="padding: 8px;">NQ</th>
                    <th colspan="4" style="padding: 8px;">MMLU</th>
                  </tr>
                  <tr>
                    <th style="padding: 8px;">Easy</th>
                    <th style="padding: 8px;">Challenge</th>
                    <th style="padding: 8px;">Humanities</th>
                    <th style="padding: 8px;">STEM</th>
                    <th style="padding: 8px;">Social Science</th>
                    <th style="padding: 8px;">Other</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td style="padding: 8px; text-align: left;">Llama-2-7B</td>
                    <td>75.51</td>
                    <td>33.57</td>
                    <td>73.95</td>
                    <td>44.28</td>
                    <td>24.02</td>
                    <td>43.27</td>
                    <td>36.09</td>
                    <td>53.04</td>
                    <td>54.84</td>
                  </tr>
                  <tr style="background-color: #e8f5e8;">
                    <td style="padding: 8px; text-align: left;">MAGNET</td>
                    <td>75.08</td>
                    <td>32.22</td>
                    <td>74.33</td>
                    <td>44.52</td>
                    <td>24.22</td>
                    <td>42.25</td>
                    <td>36.63</td>
                    <td>52.64</td>
                    <td>52.40</td>
                  </tr>
                </tbody>
              </table>              

            </details>
          </div>

        </div>
      </div>
    </div>
  </section>

  <!-- Video Tutorial -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Video Tutorial</h2>
          <div class="content">
            <video class="center" width="100%" controls>
              <source src="figures/tutorial.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </div>
    </div>
  </section> 

  <!-- BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content is-dark">
      <h2 class="title is-3">BibTex</h2>
      <pre><code>@inproceedings{khosla2025magnet,
    title={MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities},
    author={Savya Khosla and  Aditi Tiwari and Kushal Kafle and Simon Jenni and Handong Zhao and John Collomosse and Jing Shi1},
    journal={Association for Computational Linguistics},
    year={2025}
}</code></pre>
    </div>
  </section>
</body>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</html>
